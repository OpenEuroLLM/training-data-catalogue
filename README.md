# OpenEuroLLM Catalogue of LLM Training Data

## Background

There is a bit of a growth industry in (pre-)training data preparation for LLM development.
This page aims to offer navigational help in the dataset landscape, essentially providing a structured ‘catalogue’ of available resources.
Originally, the catalogue is constructed for internal use in the [OpenEuroLLM](https://openeurollm.eu/) initiative, i.e. will put most emphasis on datasets used in the project.
At the same time, we hope that this overview may become useful to others and can grow into a community-supported resource.
The catalogue will be accompanied by a curated collection of (a subset of) LLM (pre-)training datasets that shall be made available directly on multiple EuroHPC systems.

To nominate additional resources for inclusion in the catalogue or discuss specifics of emerging entries, please create a [GitHub issue on this repository](https://github.com/OpenEuroLLM/training-data-catalogue/issues).


## (Mostly) English Pre-Training Data

+ C4 ([Raffel, et al. (2019)](https://arxiv.org/abs/1910.10683))
+ The Pile
+ RefinedWeb
+ RedPajama
+ Dolma
+ **FineWeb** ([Penedo, et al. (20240)](https://arxiv.org/abs/2406.17557))
+ **DCLM** ([Li, et al. (2024)](https://arxiv.org/pdf/2406.11794))
+ **NEMOTRON-CC** ([Su, et al. (2024)](https://arxiv.org/abs/2412.02595))


## Multilingual Pre-Training Data

+ mC4
+ CulturaX
+ **[HPLT](hplt/README.md)** ([De Gilbert, et al. (2024)](https://arxiv.org/abs/2403.14009), [Burchell, et al. (2025)](https://arxiv.org/abs/2503.10267))
+ **FineWeb** (https://github.com/huggingface/fineweb-2)
+ MADLAB-400 ([Kudugunta, et al. (2023)](https://arxiv.org/abs/2309.04662))


## Non-Language Pre-Training Data

+ The Stack
